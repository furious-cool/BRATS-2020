{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aee9f8f5",
      "metadata": {
        "id": "aee9f8f5"
      },
      "outputs": [],
      "source": [
        "import nibabel as nib\n",
        "from glob import glob\n",
        "import os\n",
        "import shutil\n",
        "#import gif_your_nifti.core as gif2nif\n",
        "import math\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from skimage.util import montage \n",
        "from skimage.transform import rotate\n",
        "from scipy import ndimage, misc\n",
        "import time\n",
        "import datetime\n",
        "import csv\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install segmentation_models_3D\n",
        "!pip install volumentations-3D\n",
        "!pip install tensorflow-addons"
      ],
      "metadata": {
        "id": "uMD1NnCX3GFa",
        "collapsed": true
      },
      "id": "uMD1NnCX3GFa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import zipfile\n",
        "with zipfile.ZipFile(r\"/content/drive/MyDrive/BRATS/MICCAI_BraTS2020_TrainingData.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(r\"./content/drive/MyDrive/BRATS/MICCAI_BraTS2020_TrainingData\")\n",
        "print(glob(\"./content/drive/MyDrive/BRATS/MICCAI_BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/*\")[:2])\n",
        "\n",
        "TRAINING_DIR = r\"./content/drive/MyDrive/BRATS/MICCAI_BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData\"\n",
        "\n",
        "training_folder_list = glob(TRAINING_DIR+\"/*\")\n",
        "random.shuffle(training_folder_list)\n",
        "validation_folder_list = training_folder_list[int(0.8*(len(training_folder_list))):]\n",
        "training_folder_list = training_folder_list[:int(0.8*(len(training_folder_list)))]\n",
        "print(len(training_folder_list), len(validation_folder_list))"
      ],
      "metadata": {
        "id": "Gy0YPFjz9Dvq"
      },
      "id": "Gy0YPFjz9Dvq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "d02DTkZ2q-Cf"
      },
      "id": "d02DTkZ2q-Cf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1e0649f",
      "metadata": {
        "id": "c1e0649f"
      },
      "outputs": [],
      "source": [
        "def convert_to_gif(path):\n",
        "    os.makedirs(\"./gif\", exist_ok=True)\n",
        "    shutil.copy2(path, \"./gif/\"+os.path.basename(path))\n",
        "    gif2nif.write_gif_normal(\"./gif/\"+os.path.basename(path))\n",
        "    os.remove(\"./gif/\"+os.path.basename(path))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31c6b889",
      "metadata": {
        "scrolled": true,
        "id": "31c6b889"
      },
      "outputs": [],
      "source": [
        "def plot_mri(path):\n",
        "    figure(figsize=(15, 15), dpi=80)\n",
        "    crop_size = 40\n",
        "    dimentions= (128,128)\n",
        "    if type(path) is str:\n",
        "        mri = nib.load(path).get_fdata()\n",
        "        mri = mri[crop_size:mri.shape[0]-crop_size, crop_size:mri.shape[1]-crop_size, crop_size: mri.shape[2]-crop_size]\n",
        "        mri = ndimage.zoom(mri, 0.9)\n",
        "        print(mri.shape)\n",
        "    else:\n",
        "        mri=path\n",
        "    for index in range(mri.shape[0]):\n",
        "        plt.subplot(math.ceil(mri.shape[-1]/10),10,index+1)\n",
        "        plt.tick_params(left = False, right = False , labelleft = False ,\n",
        "                labelbottom = False, bottom = False)\n",
        "        new_mri = mri[index,...]\n",
        "        plt.imshow(new_mri,cmap='gray') \n",
        "    print(new_mri.shape)\n",
        "    plt.show()\n",
        "    \n",
        "#plot_mri(glob(training_folder_list[70]+\"/*_flair*\")[0] )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1INrWZ-JCDJZ"
      },
      "id": "1INrWZ-JCDJZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "0RmQdLDYB_Js"
      },
      "id": "0RmQdLDYB_Js",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b83898f7",
      "metadata": {
        "id": "b83898f7"
      },
      "outputs": [],
      "source": [
        "def get_filepath(paths,mod):\n",
        "    for path in paths:\n",
        "        if mod in path :\n",
        "            return path\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def bb_volume(volume, mask=None):\n",
        "    volume = np.transpose(volume,(1,2,0))\n",
        "    rows = np.where(np.max(volume, 0) > 0)[0]\n",
        "    cols = np.where(np.max(volume, 1) > 0)[0]\n",
        "    if rows.size:\n",
        "        volume = volume[cols[0]: cols[-1] + 1, rows[0]: rows[-1] + 1]\n",
        "        if mask is not None:\n",
        "            mask = np.transpose(mask,(1,2,0))\n",
        "            mask = mask[cols[0]: cols[-1] + 1, rows[0]: rows[-1] + 1]\n",
        "    else:\n",
        "        volume = volume[:1, :1]\n",
        "        if mask is not None:\n",
        "            mask = np.transpose(mask,(1,2,0))\n",
        "            mask = mask[:1, :1]\n",
        "    return np.transpose(volume,(2,0,1)), None if mask is None else np.transpose(mask,(2,0,1))\n",
        "\n",
        "def depth_filter(volume, depth):\n",
        "    mid = volume.shape[0]//2\n",
        "    return volume[mid-(depth//2):mid+(depth//2)]\n",
        "\n",
        "def plot_volume(volume):\n",
        "    fig, ax1 = plt.subplots(1, 1, figsize = (20,20))\n",
        "    ax1.imshow(rotate(montage(volume), 90, resize=True), cmap ='gray')\n",
        "\n",
        "def read(path):\n",
        "    return nib.load(path).get_fdata()\n",
        "\n",
        "def read_mri(path,stain):\n",
        "    return np.transpose(read(glob(path+\"/*\"+stain+\"*\")[0]), (2,0,1)) \n",
        "\n",
        "def clip_and_normalize(volume):\n",
        "    perc01 = np.percentile(volume, 1,  keepdims=True)\n",
        "    perc99 = np.percentile(volume, 99.995, keepdims=True)\n",
        "    mri_clipped = np.clip(volume, a_min=perc01, a_max=perc99)\n",
        "    mri_normalized = (((mri_clipped-np.min(mri_clipped)) / (np.max(mri_clipped)-np.min(mri_clipped)+1e-7))*255).astype(np.uint8)\n",
        "    return mri_normalized\n",
        "\n",
        "def resize_height_width(volume, width, height,  mask= None):\n",
        "    temp_vol , temp_mask = None, None\n",
        "    if volume is not None:\n",
        "        temp_vol = np.zeros((volume.shape[0], width, height))\n",
        "    if mask is not None:\n",
        "        temp_mask =  np.zeros((mask.shape[0], width, height))\n",
        "    \n",
        "    for index in range(volume.shape[0]):\n",
        "        if volume is not None:\n",
        "          temp_vol[index,...] = cv2.resize(volume[index], (width,height),interpolation = cv2.INTER_AREA)\n",
        "        if mask is not None:\n",
        "            temp_mask[index,...] = cv2.resize(mask[index], (width,height),interpolation = cv2.INTER_NEAREST)\n",
        "    return temp_vol , temp_mask    \n",
        "\n",
        "def create_patch_from_id_v3(folder,\n",
        "                            depth = 128,\n",
        "                            width = 128,\n",
        "                            height = 128,\n",
        "                            allowed_mod=[\"flair\",\"t1ce\"]):\n",
        "    if type(folder) is not str:\n",
        "        folder=str(folder.numpy().decode('utf-8'))\n",
        "        depth = tf.cast(depth, tf.int32)\n",
        "        width,height = tf.cast(width,tf.int32),tf.cast(height,tf.int32)\n",
        "    try:\n",
        "      mask = read_mri(folder, \"seg\")\n",
        "\n",
        "      result = []\n",
        "      for index, mod in enumerate(allowed_mod):\n",
        "        if type(mod) is not str:\n",
        "          mod = mod.numpy().decode('utf-8')\n",
        "        if index == (len(allowed_mod)-1):\n",
        "          mask_depth = depth_filter(mask,depth)\n",
        "        else:\n",
        "          mask_depth = None\n",
        "        \n",
        "        mri = read_mri(folder, mod)\n",
        "        mri_chopped, mask_chopped = bb_volume(depth_filter(mri,depth),mask_depth)\n",
        "        mri_clipped_normalized = clip_and_normalize(mri_chopped)\n",
        "        mri_resized, mask_resized = resize_height_width(mri_clipped_normalized,mask=mask_chopped, width=width, height=height)\n",
        "        result.append(mri_resized)\n",
        "      result = np.transpose(np.array(result), (1,2,3,0))\n",
        "      mask_resized[mask_resized==4] = 3\n",
        "    except:\n",
        "      return np.zeros((depth,width,height,len(allowed_mod))), np.zeros((depth,width,height,1))\n",
        "    return result, np.expand_dims(mask_resized,-1)\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "VKRvMGNPk2l2"
      },
      "id": "VKRvMGNPk2l2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cc01981",
      "metadata": {
        "id": "8cc01981",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "def create_patch_from_id_v2(path, \n",
        "                         depth_per_mod = 50,\n",
        "                         allowed_mod=[\"flair\",\"t1\",\"t1ce\", \"t2\"],\n",
        "                         dimentions=(240,240),\n",
        "                         crop_size= 40,\n",
        "                         classes = 3,\n",
        "                         frame_skip=2\n",
        "                       ):\n",
        "    if type(path) is not str:\n",
        "        path=str(path.numpy().decode('utf-8'))\n",
        "        #print(path)\n",
        "        depth_per_mod = tf.cast(depth_per_mod, tf.int32)\n",
        "        dimentions = (tf.cast(dimentions[0],tf.int32),tf.cast(dimentions[1],tf.int32))\n",
        "    paths = glob(path+\"/*\")\n",
        "    volume = np.zeros((depth_per_mod, dimentions[0],dimentions[1],len(allowed_mod)))\n",
        "    mask = np.zeros((depth_per_mod, dimentions[0],dimentions[1]))\n",
        "    #clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    try:\n",
        "      for index, mod in enumerate(allowed_mod):\n",
        "          if type(mod) is not str:\n",
        "              mod = mod.numpy().decode('utf-8')\n",
        "          path = get_filepath(paths,mod)\n",
        "          mri = nib.load(path).get_fdata() \n",
        "          mri = mri[crop_size:mri.shape[0]-crop_size, crop_size:mri.shape[1]-crop_size, crop_size: mri.shape[2]-crop_size]\n",
        "          #mri = ndimage.zoom(mri, 0.9,order=0)\n",
        "          rows = np.where(np.max(mri, 0) > 0)[0]\n",
        "          cols = np.where(np.max(mri, 1) > 0)[0]\n",
        "          if rows.size:\n",
        "              mri = mri[cols[0]: cols[-1] + 1, rows[0]: rows[-1] + 1]\n",
        "          else:\n",
        "              mri = mri[:1, :1]\n",
        "          mid = mri.shape[-1]//2\n",
        "          count=0\n",
        "          for i in range(mid-(frame_skip*(depth_per_mod//2)),mid+(frame_skip*(depth_per_mod//2)),frame_skip):\n",
        "              temp = cv2.resize(mri[...,i], dimentions,interpolation = cv2.INTER_AREA)\n",
        "              temp = (((temp-np.min(temp)) / (np.max(temp)-np.min(temp)+1e-7))*255).astype(np.uint8)\n",
        "              volume[count,...,index] = temp\n",
        "              count+=1\n",
        "      perc01 = np.percentile(volume, 1,  keepdims=True)\n",
        "      perc99 = np.percentile(volume, 99, keepdims=True)\n",
        "\n",
        "      #  Clip array with different limits across the z dimension\n",
        "      volume = np.clip(volume, a_min=perc01, a_max=perc99)\n",
        "\n",
        "      path = get_filepath(paths,\"seg\")\n",
        "      mri = nib.load(path).get_fdata()\n",
        "      mri = mri[crop_size:mri.shape[0]-crop_size, crop_size:mri.shape[1]-crop_size, crop_size: mri.shape[2]-crop_size]\n",
        "      #mri = ndimage.zoom(mri, 0.9,order=0)\n",
        "      if rows.size:\n",
        "          mri = mri[cols[0]: cols[-1] + 1, rows[0]: rows[-1] + 1]\n",
        "      else:\n",
        "          mri = mri[:1, :1]\n",
        "      mid = mri.shape[-1]//2\n",
        "      count=0\n",
        "      for i in range(mid-(frame_skip*(depth_per_mod//2)),mid+(frame_skip*(depth_per_mod//2)),frame_skip):\n",
        "          mask[count,...] = cv2.resize(mri[...,i], dimentions,interpolation =  cv2.INTER_NEAREST)\n",
        "          count+=1\n",
        "      mask[mask==4] = 3;\n",
        "    except:\n",
        "      pass\n",
        "      #raise\n",
        "    #print(np.unique(mask))\n",
        "\n",
        "    return volume, np.expand_dims(mask,-1)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c0f4b16",
      "metadata": {
        "id": "1c0f4b16"
      },
      "outputs": [],
      "source": [
        "volume,mask = create_patch_from_id_v3(training_folder_list[38], \n",
        "                         depth = 96,\n",
        "                         allowed_mod=[\"flair\",'t1ce','t2'],\n",
        "                         height = 64,\n",
        "                         width = 64)\n",
        "print(volume.shape, np.max(volume), np.min(volume))\n",
        "print(mask.shape,np.unique(mask))\n",
        "fig, ax1 = plt.subplots(1, 1, figsize = (15,15))\n",
        "ax1.imshow(rotate(montage(volume[:,:,:,2]), 90, resize=True), cmap ='gray')\n",
        "#fig, ax1 = plt.subplots(1, 1, figsize = (15,15))\n",
        "#ax1.imshow(rotate(montage(volume[:,:,:,1]), 90, resize=True), cmap ='gray')\n",
        "#fig, ax1 = plt.subplots(1, 1, figsize = (15,25))\n",
        "#ax1.imshow(rotate(montage(volume[:,:,:,2]), 90, resize=True), cmap ='gray')\n",
        "fig, ax1 = plt.subplots(1, 1, figsize = (15,25))\n",
        "ax1.imshow(rotate(montage(mask[:,:,:,0]), 90, resize=True), cmap ='gray')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def random_saturation(image, mask):\n",
        "  rd = tf.random.uniform([],0,1.0)\n",
        "  sat_cond = tf.less(rd, 0.5)\n",
        "  new_image = tf.cond(sat_cond, lambda: tf.image.random_saturation(image, 0.75,1.25), lambda:image)\n",
        "  return new_image, mask\n",
        "\n",
        "def random_brightness(image, mask):\n",
        "  rd = tf.random.uniform([],0,1.0)\n",
        "  sat_cond = tf.less(rd, 0.5)\n",
        "  new_image = tf.cond(sat_cond, lambda: tf.image.random_brightness(image, 0.20), lambda:image)\n",
        "  return new_image, mask\n",
        "\n",
        "def random_contrast(image, mask):\n",
        "  rd = tf.random.uniform([],0,1.0)\n",
        "  sat_cond = tf.less(rd, 0.5)\n",
        "  new_image = tf.cond(sat_cond, lambda: tf.image.random_contrast(image, 0.75,1.75), lambda:image)\n",
        "  return new_image, mask\n",
        "\n",
        "def normalize_function(image, mask):\n",
        "  image = tf.cast(image,dtype=tf.float16)\n",
        "  image = tf.truediv(image, tf.reduce_max(image)+1e-7)\n",
        "  image = tf.cast(image,dtype=tf.float16)\n",
        "  return image, mask"
      ],
      "metadata": {
        "id": "YdpA3QEykwvR"
      },
      "id": "YdpA3QEykwvR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d977b8f9",
      "metadata": {
        "id": "d977b8f9"
      },
      "outputs": [],
      "source": [
        "depth_per_mod = 128\n",
        "allowed_mod=[\"flair\",'t1ce']\n",
        "dimentions=(128,128)\n",
        "classes=3\n",
        "crop_size= 0\n",
        "frame_skip=1\n",
        "\n",
        "def pyfunc_create_patch_from_id(path):\n",
        "    depth_per_mod_g = depth_per_mod\n",
        "    allowed_mod_g=allowed_mod\n",
        "    dimentions_g=dimentions\n",
        "    classes_g=classes\n",
        "    crop_size_g= crop_size\n",
        "    frame_skip_g=frame_skip\n",
        "\n",
        "    volume, mask = tf.py_function(\n",
        "        create_patch_from_id_v3, \n",
        "            [path,\n",
        "            depth_per_mod_g,\n",
        "            dimentions_g[0],\n",
        "            dimentions_g[1],\n",
        "            allowed_mod_g], [tf.float32, tf.uint8]\n",
        "    )\n",
        "    return volume, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a05185ef",
      "metadata": {
        "id": "a05185ef"
      },
      "outputs": [],
      "source": [
        "from volumentations import *\n",
        "def one_hot(x,y):\n",
        "    x.set_shape([depth_per_mod, dimentions[0],dimentions[1],len(allowed_mod)])\n",
        "    y_new = tf.one_hot(tf.squeeze(y, axis=-1),4)\n",
        "    y_new=y_new[...,1:]\n",
        "    y_new.set_shape([depth_per_mod, dimentions[0],dimentions[1],classes])\n",
        "    return x,y_new\n",
        "  \n",
        "def get_augmentation():\n",
        "    return Compose([\n",
        "        Rotate((-10,10), (0, 0), (0,0), p=0.5),\n",
        "        GaussianNoise(var_limit=(0, 15), p=0.5),\n",
        "        RandomGamma(gamma_limit=(0.7, 1.3), p=0.5),\n",
        "        RandomRotate90((1, 2), p=0.5)\n",
        "    ], p=0.5)\n",
        "\n",
        "\n",
        "def augmentation(image, mask):\n",
        "    image =  tf.cast(image, tf.float32).numpy()\n",
        "    mask = tf.cast(mask, tf.uint8).numpy()\n",
        "    data = {'image': image,\"mask\": mask }\n",
        "    compose = get_augmentation()\n",
        "    aug_data = compose(**data)\n",
        "    image_aug = aug_data['image']\n",
        "    mask_aug = aug_data['mask']\n",
        "    return image_aug, mask_aug\n",
        "\n",
        "\n",
        "def pyfunc_augmentation(image, mask):\n",
        "  image, mask = tf.py_function(\n",
        "        augmentation, \n",
        "            [image, mask], [tf.float32, tf.uint8]\n",
        "    )\n",
        "  return image,mask"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "zQ8ug_8Q9L0E"
      },
      "id": "zQ8ug_8Q9L0E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40e700b7",
      "metadata": {
        "id": "40e700b7"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices(training_folder_list)\n",
        "train_dataset = train_dataset.repeat()\n",
        "train_dataset = train_dataset.shuffle(100, reshuffle_each_iteration=True)\n",
        "train_dataset = train_dataset.map(pyfunc_create_patch_from_id, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_dataset = train_dataset.map(pyfunc_augmentation,num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_dataset = train_dataset.map(one_hot,num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "train_dataset = train_dataset.map(normalize_function,num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "train_dataset = train_dataset.batch(1)\n",
        "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices(validation_folder_list)\n",
        "valid_dataset = valid_dataset.repeat()\n",
        "valid_dataset = valid_dataset.shuffle(100,reshuffle_each_iteration=True)\n",
        "valid_dataset = valid_dataset.map(pyfunc_create_patch_from_id, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "valid_dataset = valid_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "valid_dataset = valid_dataset.map(one_hot,num_parallel_calls=tf.data.AUTOTUNE)\n",
        "valid_dataset = valid_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "valid_dataset = valid_dataset.map(normalize_function,num_parallel_calls=tf.data.AUTOTUNE)\n",
        "valid_dataset = valid_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "valid_dataset = valid_dataset.batch(1)\n",
        "valid_dataset = valid_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for volume, mask in train_dataset.as_numpy_iterator():\n",
        "    print(volume.shape, np.max(volume), np.min(volume))\n",
        "    print(mask.shape, np.unique(mask))\n",
        "\n",
        "    fig, ax1 = plt.subplots(1, 1, figsize = (20,20))\n",
        "    ax1.imshow(rotate(montage(volume[0,:,:,:,0].astype(np.float32)), 90, resize=True), cmap ='gray')\n",
        "\n",
        "    fig, ax1 = plt.subplots(1, 1, figsize = (20,20))\n",
        "    ax1.imshow(rotate(montage(mask[0,:,:,:,1]), 90, resize=True), cmap ='gray')\n",
        "    break"
      ],
      "metadata": {
        "id": "2eGpPxLQ-0-H"
      },
      "id": "2eGpPxLQ-0-H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras.backend as K\n",
        "def scheduler(epoch, lr):\n",
        "  #if ((epoch+1) % 4 == 0) and (lr > 0.00005):\n",
        "  if False:\n",
        "    return lr * tf.math.exp(-0.4)\n",
        "  else:\n",
        "    return lr\n",
        "\n",
        "def custom_CE_loss_function(weights = [1,1,1]):\n",
        "    def weighted_categorical_crossentropy(target, output):\n",
        "        output /= K.sum(output, axis=-1, keepdims=True)\n",
        "        output = K.clip(output, 1e-6, 1 - 1e-6)\n",
        "        loss_per_pixel = K.sum((target * -K.log(output+1e-6))*weights, axis=-1, keepdims=False)\n",
        "        weighted_loss = K.mean(loss_per_pixel)\n",
        "        return weighted_loss\n",
        "    return weighted_categorical_crossentropy\n",
        "    \n",
        "def focal_cross_entropy():\n",
        "    loss = tf.keras.losses.BinaryFocalCrossentropy(gamma=2.0, from_logits=False)\n",
        "    def focal_loss(y_true, y_pred):\n",
        "      result = 0\n",
        "      for i in range(classes):\n",
        "          y_pred = K.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "          if i==0:\n",
        "            result = loss(y_true[...,i], y_pred[...,i])\n",
        "          else:\n",
        "            result += (loss(y_true[...,i], y_pred[...,i]))\n",
        "          #print(result)\n",
        "      return result\n",
        "    return focal_loss\n",
        "\n",
        "def dice_coef(y_true, y_pred, smooth=1e-7):\n",
        "    class_num = 3\n",
        "    for i in range(class_num):\n",
        "        y_true_f = K.flatten(y_true[...,i])\n",
        "        y_pred_f = K.flatten(y_pred[...,i])\n",
        "        intersection = K.sum(y_true_f * y_pred_f)\n",
        "        loss = ((2. * intersection + smooth) / (K.sum(y_true_f*y_true_f) + K.sum(y_pred_f*y_pred_f) + smooth))\n",
        "        if i == 0:\n",
        "            total_loss = loss\n",
        "        else:\n",
        "            total_loss = total_loss + loss\n",
        "        #print(loss)\n",
        "    total_loss = total_loss / class_num\n",
        "    return total_loss\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    return 1 - dice_coef(y_true, y_pred)\n",
        "    \n",
        "def CE_Diceloss(y_true, y_pred):\n",
        "    ce_loss = custom_CE_loss_function([1,1,1])\n",
        "    return dice_loss(y_true, y_pred) + ce_loss(y_true, y_pred)\n",
        "\n",
        "def dice_focal_loss(y_true, y_pred):\n",
        "  fc = focal_cross_entropy()\n",
        "  return dice_loss(y_true, y_pred) + fc(y_true, y_pred)\n",
        "\n",
        "def dice_coef_necrotic(y_true, y_pred, epsilon=1e-6):\n",
        "    #y_pred = tf.cast(y_pred > 0.5, tf.float32)\n",
        "    intersection = K.sum(y_true[...,0] * y_pred[...,0])\n",
        "    return (2. * intersection) / (K.sum(y_true[...,0]) + K.sum(y_pred[...,0]) + epsilon)\n",
        "\n",
        "def dice_coef_edema(y_true, y_pred, epsilon=1e-6):\n",
        "    #y_pred = tf.cast(y_pred > 0.5, tf.float32)\n",
        "    intersection = K.sum(y_true[...,1] * y_pred[...,1])\n",
        "    return (2. * intersection) / (K.sum(y_true[...,1]) + K.sum(y_pred[...,1]) + epsilon)\n",
        "\n",
        "def dice_coef_enhancing(y_true, y_pred, epsilon=1e-6):\n",
        "    #y_pred = tf.cast(y_pred > 0.5, tf.float32)\n",
        "    intersection = K.sum(y_true[...,2] * y_pred[...,2])\n",
        "    return (2. * intersection) / (K.sum(y_true[...,2]) + K.sum(y_pred[...,2]) + epsilon)\n",
        "\n",
        "def precision_necrotic(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true[...,0] * y_pred[...,0], 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred[...,0], 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "def precision_edema(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true[...,1] * y_pred[...,1], 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred[...,1], 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "\n",
        "def precision_enhancing(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true[...,2] * y_pred[...,2], 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred[...,2], 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Computing Sensitivity      \n",
        "def sensitivity_necrotic(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true[...,0] * y_pred[...,0], 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true[...,0], 0, 1)))\n",
        "    return true_positives / (possible_positives + K.epsilon())\n",
        "\n",
        "def sensitivity_edema(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true[...,1] * y_pred[...,1], 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true[...,1], 0, 1)))\n",
        "    return true_positives / (possible_positives + K.epsilon())\n",
        "\n",
        "def sensitivity_enhancing(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true[...,2] * y_pred[...,2], 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true[...,2], 0, 1)))\n",
        "    return true_positives / (possible_positives + K.epsilon())\n",
        "\n",
        "\n",
        "# Computing Specificity\n",
        "def specificity_necrotic(y_true, y_pred):\n",
        "    true_negatives = K.sum(K.round(K.clip((1-y_true[...,0]) * (1-y_pred[...,0]), 0, 1)))\n",
        "    possible_negatives = K.sum(K.round(K.clip(1-y_true[...,0], 0, 1)))\n",
        "    return true_negatives / (possible_negatives + K.epsilon())\n",
        "\n",
        "def specificity_edema(y_true, y_pred):\n",
        "    true_negatives = K.sum(K.round(K.clip((1-y_true[...,1]) * (1-y_pred[...,1]), 0, 1)))\n",
        "    possible_negatives = K.sum(K.round(K.clip(1-y_true[...,1], 0, 1)))\n",
        "    return true_negatives / (possible_negatives + K.epsilon())\n",
        "\n",
        "def specificity_enhancing(y_true, y_pred):\n",
        "    true_negatives = K.sum(K.round(K.clip((1-y_true[...,2]) * (1-y_pred[...,2]), 0, 1)))\n",
        "    possible_negatives = K.sum(K.round(K.clip(1-y_true[...,2], 0, 1)))\n",
        "    return true_negatives / (possible_negatives + K.epsilon())\n"
      ],
      "metadata": {
        "id": "U65ilIfNJLaq"
      },
      "id": "U65ilIfNJLaq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_addons as tfa\n",
        "def Ranger(sync_period=6,\n",
        "           slow_step_size=0.5,\n",
        "           learning_rate=5e-5,\n",
        "           beta_1=0.9,\n",
        "           beta_2=0.999,\n",
        "           epsilon=1e-7,\n",
        "           weight_decay=0.,\n",
        "           amsgrad=False,\n",
        "           sma_threshold=5.0,\n",
        "           total_steps=0,\n",
        "           warmup_proportion=0.3,\n",
        "           min_lr=0.,\n",
        "           name=\"Ranger\"):\n",
        "    inner = tfa.optimizers.RectifiedAdam(learning_rate, beta_1, beta_2, epsilon, weight_decay, amsgrad, sma_threshold, total_steps, warmup_proportion, min_lr, name)\n",
        "    optim = tfa.optimizers.Lookahead(inner, sync_period, slow_step_size, name)\n",
        "    return optim"
      ],
      "metadata": {
        "id": "sW79kO3DRU78"
      },
      "id": "sW79kO3DRU78",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fac0e464",
      "metadata": {
        "id": "fac0e464"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "#keras.backend.set_image_data_format('channels_last')\n",
        "import segmentation_models_3D as sm\n",
        "custom_model = sm.Unet('efficientnetb2', input_shape=(128, 128, 128, len(allowed_mod)), encoder_weights=None, classes=classes, activation='sigmoid')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "load = True\n",
        "if load:\n",
        "  weight_path = \"/content/drive/MyDrive/BRATS/models/v2_1646179339/basic_customRangerLoss-35-0.664.hdf5\"\n",
        "  custom_model.load_weights(weight_path)\n",
        "  model_save_path = os.path.dirname(weight_path)\n",
        "  logdir = \"/content/drive/MyDrive/BRATS/models/v2_1646179339/logs20220302-0002120\"\n",
        "  stats_save_path = model_save_path+\"/history.csv\"\n",
        "  csvfile = open(stats_save_path, 'a')\n",
        "  csv_writer = csv.writer(csvfile)\n",
        "  headers = True\n",
        "else:\n",
        "  model_save_path = '/content/drive/MyDrive/BRATS/models/v2_'+str(int(time.time()))\n",
        "  stats_save_path = model_save_path+\"/history.csv\"\n",
        "  os.mkdir(model_save_path)\n",
        "  logdir = model_save_path +\"/logs/\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  csvfile = open(stats_save_path, 'a')\n",
        "  csv_writer = csv.writer(csvfile) \n",
        "  headers = False"
      ],
      "metadata": {
        "id": "2zKV4NGSC1gI"
      },
      "id": "2zKV4NGSC1gI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4308394",
      "metadata": {
        "id": "c4308394"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
        "                              patience=3, min_lr=0.00001)\n",
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "CE_loss = custom_CE_loss_function([1,1,1])\n",
        "\n",
        "custom_model.compile(optimizer=opt, loss=dice_focal_loss, metrics = [\"accuracy\",\n",
        "                                                                      dice_loss,\n",
        "                                                                      focal_cross_entropy(),\n",
        "                                                                      tf.keras.metrics.OneHotMeanIoU(num_classes=3),\n",
        "                                                                      precision_necrotic,\n",
        "                                                                      precision_edema,\n",
        "                                                                      precision_enhancing,\n",
        "                                                                      sensitivity_necrotic,\n",
        "                                                                      sensitivity_edema,\n",
        "                                                                      sensitivity_enhancing,\n",
        "                                                                      specificity_necrotic,\n",
        "                                                                      specificity_edema,\n",
        "                                                                      specificity_enhancing,\n",
        "                                                                      dice_coef_necrotic, \n",
        "                                                                      dice_coef_edema ,\n",
        "                                                                      dice_coef_enhancing])\n",
        "\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=model_save_path+ \"/basic_customRangerLoss-{epoch:02d}-{val_loss:.3f}.hdf5\",\n",
        "    save_weights_only=True,\n",
        "    monitor = \"val_loss\",\n",
        "    mode='min',save_best_only=True,\n",
        "    save_freq='epoch')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1df0cdad",
      "metadata": {
        "id": "1df0cdad"
      },
      "outputs": [],
      "source": [
        "history = custom_model.fit(train_dataset,\n",
        "                    epochs=40 ,\n",
        "                    steps_per_epoch=200,\n",
        "                    validation_data = valid_dataset,\n",
        "                    validation_steps=70, \n",
        "                    callbacks=[lr , model_checkpoint_callback, tensorboard_callback])\n",
        "if not headers:\n",
        "  csv_writer.writerow(list(history.history.keys()))\n",
        "  headers = True\n",
        "csv_writer.writerows(list(np.array(list(history.history.values())).T))\n",
        "csvfile.flush()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " \n",
        "import pandas as pd\n",
        "df_history = pd.DataFrame(history.history)\n",
        "df_history.to_csv(\"BASE_MODEl_v24_with_augmentation_ramger_loss_v2.csv\")"
      ],
      "metadata": {
        "id": "dLaz04HB9KEo"
      },
      "id": "dLaz04HB9KEo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BxaW1_L5gQ-4"
      },
      "id": "BxaW1_L5gQ-4"
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir \"/content/drive/MyDrive/BRATS/models/v2_1646179339/logs/20220302-000219\""
      ],
      "metadata": {
        "id": "U7ephzf0gQWI"
      },
      "id": "U7ephzf0gQWI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RKVjXUF90jqG"
      },
      "id": "RKVjXUF90jqG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Es8F3MHrgO7v"
      },
      "id": "Es8F3MHrgO7v"
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "import segmentation_models_3D as sm\n",
        "model_test = sm.Unet('efficientnetb3', input_shape=(64, 128, 128, len(allowed_mod)), encoder_weights=None, classes=classes, activation='softmax')\n",
        "model_test.load_weights('/content/baisc_v20_customCELoss-04-0.911.hdf5')"
      ],
      "metadata": {
        "id": "h6XShOOvkLvK"
      },
      "id": "h6XShOOvkLvK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_volume(volume,mask):\n",
        "    #print(volume.shape, mask.shape)\n",
        "    fig, ax1 = plt.subplots(1, 1, figsize = (20,15))\n",
        "    plt.subplot(1,2,1)\n",
        "    \n",
        "    plt.xticks([]),plt.yticks([])\n",
        "    plt.imshow(rotate(montage(volume), 90, resize=True), cmap ='gray')\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.xticks([]),plt.yticks([])\n",
        "    plt.imshow(rotate(montage(mask), 90, resize=True), cmap ='gray')\n"
      ],
      "metadata": {
        "id": "dQ9BQD2OGK2j"
      },
      "id": "dQ9BQD2OGK2j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count, ne,ed,eh, cel = 0,0,0,0,0\n",
        "x,y = None,None\n",
        "for volume, mask in train_dataset.as_numpy_iterator():\n",
        "  if count < 1:\n",
        "      count+=1\n",
        "  else:\n",
        "      break\n",
        "  print(volume.shape, np.unique(mask))\n",
        "  print(mask.shape, np.unique(mask))\n",
        "  result = custom_model.predict(np.expand_dims(volume[0,...],0))\n",
        "  x,y = mask,result\n",
        "  plot_volume(mask[0,...,0], result[0,...,0]>0.5)\n",
        "  plot_volume(mask[0,...,1], result[0,...,1]>0.5)\n",
        "  plot_volume(mask[0,...,2], result[0,...,2]>0.5)\n",
        "  #plot_volume()\n",
        "  print(\"mask\",np.unique(np.argmax(np.expand_dims(mask[0,...],0),axis=-1),return_counts=True))\n",
        "  print(\"result\",np.unique(np.argmax(result,axis=-1),return_counts=True))\n",
        "  ne+= dice_coef_necrotic(mask, result).numpy() \n",
        "  ed +=dice_coef_edema(mask, result).numpy()\n",
        "  eh += dice_coef_enhancing(mask, result).numpy()\n",
        "  cel+=CE_loss(mask, result).numpy()\n",
        "  print(dice_coef_necrotic(mask, result).numpy().round(3),\n",
        "      dice_coef_edema(mask, result).numpy().round(3),\n",
        "      dice_coef_enhancing(mask, result).numpy().round(3),\n",
        "      CE_loss(mask, result).numpy().round(3)\n",
        "      )\n",
        "print(ne/10, ed/10, eh/10, cel/10)\n",
        "    "
      ],
      "metadata": {
        "id": "L-32PeE5vEF5"
      },
      "id": "L-32PeE5vEF5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_volume(x[0,...,0], y[0,...,0])"
      ],
      "metadata": {
        "id": "jW7oMlyqdhSA"
      },
      "id": "jW7oMlyqdhSA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y[0,...,0]"
      ],
      "metadata": {
        "id": "X6HIQUnf0lIE"
      },
      "id": "X6HIQUnf0lIE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_mri(result[0,...,2]>0.5)\n",
        "print(np.unique(np.argmax(result[0,...],axis=-1),return_counts=True))\n",
        "print(np.unique(result[0,...,2]>0.5, return_counts=True))"
      ],
      "metadata": {
        "id": "vinlmLhwTw0-"
      },
      "id": "vinlmLhwTw0-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp1, temp2, temp3 = None,None, None\n",
        "for path in training_folder_list[29:30]:\n",
        "  try:\n",
        "    print(os.path.basename(path))\n",
        "    volume,mask = create_patch_from_id_v3(path, \n",
        "                         depth = 128,\n",
        "                         allowed_mod=[\"flair\",'t1ce'],\n",
        "                         height = 128,\n",
        "                         width = 128\n",
        "                        )\n",
        "    #mask = np.squeeze(np.eye(4)[mask.reshape(-1)])\n",
        "    volume = volume/(np.max(volume)+1e-7)\n",
        "    temp1=volume\n",
        "    #plot_mri(volume[...,0])\n",
        "    result = custom_model.predict(np.expand_dims(volume,0))\n",
        "    temp3= result \n",
        "    #print(\"mask\",np.unique(mask,return_counts=True))\n",
        "    print(\"result\",np.unique(np.argmax(result,axis=-1),return_counts=True))\n",
        "    mask = tf.keras.utils.to_categorical(mask, dtype =\"float32\")\n",
        "    temp2=mask[...,1:]\n",
        "    print(dice_coef_necrotic(mask[...,1:], result).numpy(),\n",
        "      dice_coef_edema(mask[...,1:], result).numpy(),\n",
        "      dice_coef_enhancing(mask[...,1:], result).numpy()\n",
        "      )\n",
        "  except:\n",
        "    raise"
      ],
      "metadata": {
        "id": "zFt7myOCNidt"
      },
      "id": "zFt7myOCNidt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(temp2.shape, temp3.shape)\n",
        "plot_volume(temp2[...,0], temp3[0,...,0])"
      ],
      "metadata": {
        "id": "AlFr2znRgSJa"
      },
      "id": "AlFr2znRgSJa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m1= np.expand_dims(temp2[45,...,0],-1)\n",
        "m2= np.expand_dims(temp2[45,...,1],-1)\n",
        "m3= np.expand_dims(temp2[45,...,2],-1)\n",
        "\n",
        "p1 = np.expand_dims(temp3[0,45,...,0],-1)\n",
        "p2 = np.expand_dims(temp3[0,45,...,1],-1)\n",
        "p3 = np.expand_dims(temp3[0,45,...,2],-1)\n",
        "\n",
        "v = np.expand_dims(temp1[45,...,0],-1)\n",
        "\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "figure(figsize=(9, 12), dpi=80)\n",
        "plt.subplot(3,2,1)\n",
        "plt.xticks([]),plt.yticks([])\n",
        "green = np.ones(m1.shape, dtype=np.float)*(0,1,0)\n",
        "out = green*m1 + v*(1.0-m1)\n",
        "plt.imshow(out)\n",
        "plt.subplot(3,2,2)\n",
        "plt.xticks([]),plt.yticks([])\n",
        "green = np.ones(p1.shape, dtype=np.float)*(0,1,0)\n",
        "out = green*p1 + v*(1.0-p1)\n",
        "plt.imshow(out)\n",
        "\n",
        "plt.subplot(3,2,3)\n",
        "plt.xticks([]),plt.yticks([])\n",
        "green = np.ones(m2.shape, dtype=np.float)*(0,1,0)\n",
        "out = green*m2 + v*(1.0-m2)\n",
        "plt.imshow(out)\n",
        "\n",
        "plt.subplot(3,2,4)\n",
        "plt.xticks([]),plt.yticks([])\n",
        "green = np.ones(p2.shape, dtype=np.float)*(0,1,0)\n",
        "out = green*p2 + v*(1.0-p2)\n",
        "plt.imshow(out)\n",
        "\n",
        "plt.subplot(3,2,5)\n",
        "plt.xticks([]),plt.yticks([])\n",
        "green = np.ones(m3.shape, dtype=np.float)*(0,1,0)\n",
        "out = green*m3 + v*(1.0-m3)\n",
        "plt.imshow(out)\n",
        "\n",
        "plt.subplot(3,2,6)\n",
        "plt.xticks([]),plt.yticks([])\n",
        "green = np.ones(p3.shape, dtype=np.float)*(0,1,0)\n",
        "out = green*p3 + v*(1.0-p3)\n",
        "plt.imshow(out)\n"
      ],
      "metadata": {
        "id": "DWzdhlWErn2V"
      },
      "id": "DWzdhlWErn2V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5wUgA8Xgb4EJ"
      },
      "id": "5wUgA8Xgb4EJ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "BRATS.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}